{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bbecd4",
   "metadata": {},
   "source": [
    "# Pyspark Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b83ebf-5191-4cb4-bb6d-37e98f15c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, udf\n",
    "from pyspark.sql.types import DoubleType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e4766",
   "metadata": {},
   "source": [
    "## Building SparkSession and loadting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c167c9-c58a-4b17-a453-c9ff4edc621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 08:21:49 WARN Utils: Your hostname, Anmols-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.4.2.150 instead (on interface en0)\n",
      "24/11/05 08:21:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 08:21:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bitcoin Transactions Analysis\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Spark log level to ERROR to suppress warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"C:/Users/91628/Desktop/CLASSROOM/Semester 3/Big Data Framework/Week 7/Assignment/bitcoin_transactions.csv\"  # Replace with the actual path if downloaded locally or in a cloud bucket\n",
    "\n",
    "# Load the data with appropriate partitioning (adjust partition size based on data size and environment)\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08464bfc-4439-41ea-bb69-0f9013c61497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trade_id: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- side: string (nullable = true)\n",
      " |-- volume(quote): double (nullable = true)\n",
      " |-- size(base): double (nullable = true)\n",
      "\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "|          trade_id|    timestamp|   price|side|volume(quote)|size(base)|\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "|728569744763912192|1609430484000|28814.99| buy|   2103.49427|     0.073|\n",
      "|728574157171720192|1609431536000| 28627.2| buy|     74.43072|    0.0026|\n",
      "|728708911770632192|1609463664000| 29335.0|sell|    1041.3925|    0.0355|\n",
      "|728711638068232192|1609464314000|29301.46| buy|    102.55511|    0.0035|\n",
      "|728713911381000192|1609464856000|29345.12| buy|     88.03536|     0.003|\n",
      "|728720152505352192|1609466344000|29390.18| buy|   111.682684|    0.0038|\n",
      "|728725932256264192|1609467722000|29310.74| buy|  2142.615094|    0.0731|\n",
      "|728726825643016192|1609467935000|29348.01| buy|     2.934801|    1.0E-4|\n",
      "|728741895777288192|1609471528000| 29260.0|sell|      318.934|    0.0109|\n",
      "|728766495370248192|1609477393000|29171.31| buy|   107.933847|    0.0037|\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema of the dataset\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first 10 rows\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9ce355b-b3df-49c2-9f07-3bfa2006c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:================================================>        (22 + 4) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 52345506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows\n",
    "total_rows = df.count()\n",
    "print(f\"Total number of rows: {total_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000260d-77b8-43d4-8513-2272084e36a0",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac8128-aeb2-4518-af21-dc27b899c88b",
   "metadata": {},
   "source": [
    "#### Handling missing values: Remove or impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c8bc8b-8ce7-4b8b-8f6a-cd9bd70bfd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before dropping missing values: 52345506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:================================================>        (22 + 4) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after dropping missing values: 52345506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 1: Handling Missing Values\n",
    "# Count rows before dropping missing values\n",
    "initial_count = df.count()\n",
    "print(f\"Row count before dropping missing values: {initial_count}\")\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Count rows after dropping missing values\n",
    "after_nan_count = df_cleaned.count()\n",
    "print(f\"Row count after dropping missing values: {after_nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b22aeb92-049d-4999-938f-624becdf003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before removing duplicates: 52345506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after removing duplicates: 52345504\n",
      "First 10 rows after removing duplicates:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "|          trade_id|    timestamp|   price|side|volume(quote)|size(base)|\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "|729911318064136195|1609750340000|32033.83| buy|   634.269834|    0.0198|\n",
      "|729934147660808194|1609755783000| 28700.0|sell|      1854.02|    0.0646|\n",
      "|732579646188552210|1610386519000|33098.71| buy|   433.593101|    0.0131|\n",
      "|732902582430728192|1610463513000|33408.74|sell|    651.47043|    0.0195|\n",
      "|733214286326792193|1610537829000|35196.17| buy|   239.333956|    0.0068|\n",
      "|734663925869576193|1610883450000|35120.83| buy|    17.560415|    5.0E-4|\n",
      "|735050342902792192|1610975579000|36877.94| buy|   999.392174|    0.0271|\n",
      "|735076490193928193|1610981813000|36400.01| buy|    47.320013|    0.0013|\n",
      "|736144355798024193|1611236412000| 32561.0| buy|     182.3416|    0.0056|\n",
      "|736406051007496192|1611298805000|31556.51|sell|   211.428617|    0.0067|\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 2: Dealing with Duplicate Records\n",
    "# Count rows before removing duplicates\n",
    "count_before_duplicates = df_cleaned.count()\n",
    "print(f\"Row count before removing duplicates: {count_before_duplicates}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_no_duplicates = df_cleaned.dropDuplicates()\n",
    "\n",
    "# Count rows after removing duplicates\n",
    "count_after_duplicates = df_no_duplicates.count()\n",
    "print(f\"Row count after removing duplicates: {count_after_duplicates}\")\n",
    "\n",
    "# Show the first 10 rows after removing duplicates\n",
    "print(\"First 10 rows after removing duplicates:\")\n",
    "df_no_duplicates.show(10)  # Display the DataFrame as a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e53109-a077-468c-95ef-6d4c7a443f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema before conversion:\n",
      "root\n",
      " |-- trade_id: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- side: string (nullable = true)\n",
      " |-- volume(quote): double (nullable = true)\n",
      " |-- size(base): double (nullable = true)\n",
      "\n",
      "Schema after conversion:\n",
      "root\n",
      " |-- trade_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- side: string (nullable = true)\n",
      " |-- volume(quote): double (nullable = true)\n",
      " |-- size(base): double (nullable = true)\n",
      "\n",
      "First 10 rows after data type conversion:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+----+-------------+----------+\n",
      "|          trade_id|          timestamp|   price|side|volume(quote)|size(base)|\n",
      "+------------------+-------------------+--------+----+-------------+----------+\n",
      "|729911318064136195|2021-01-04 03:52:20|32033.83| buy|   634.269834|    0.0198|\n",
      "|729934147660808194|2021-01-04 05:23:03| 28700.0|sell|      1854.02|    0.0646|\n",
      "|732579646188552210|2021-01-11 12:35:19|33098.71| buy|   433.593101|    0.0131|\n",
      "|732902582430728192|2021-01-12 09:58:33|33408.74|sell|    651.47043|    0.0195|\n",
      "|733214286326792193|2021-01-13 06:37:09|35196.17| buy|   239.333956|    0.0068|\n",
      "|734663925869576193|2021-01-17 06:37:30|35120.83| buy|    17.560415|    5.0E-4|\n",
      "|735050342902792192|2021-01-18 08:12:59|36877.94| buy|   999.392174|    0.0271|\n",
      "|735076490193928193|2021-01-18 09:56:53|36400.01| buy|    47.320013|    0.0013|\n",
      "|736144355798024193|2021-01-21 08:40:12| 32561.0| buy|     182.3416|    0.0056|\n",
      "|736406051007496192|2021-01-22 02:00:05|31556.51|sell|   211.428617|    0.0067|\n",
      "+------------------+-------------------+--------+----+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "# Step 3: Data Type Conversion\n",
    "# Show schema before conversion\n",
    "print(\"Schema before conversion:\")\n",
    "df_no_duplicates.printSchema()\n",
    "\n",
    "# Convert 'timestamp' to timestamp and 'trade_id' to string\n",
    "df_converted = df_no_duplicates \\\n",
    "    .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\") / 1000)) \\\n",
    "    .withColumn(\"trade_id\", col(\"trade_id\").cast(\"string\"))\n",
    "\n",
    "# Show schema after conversion\n",
    "print(\"Schema after conversion:\")\n",
    "df_converted.printSchema()\n",
    "\n",
    "# Show the first 10 rows after conversion\n",
    "print(\"First 10 rows after data type conversion:\")\n",
    "df_converted.show(10)  # Display the DataFrame as a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33d6e89d-dbc5-4e24-b9b0-df00b0349028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invalid rows found: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count after filtering invalid rows: 52345504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+----+-------------+----------+\n",
      "|          trade_id|          timestamp|   price|side|volume(quote)|size(base)|\n",
      "+------------------+-------------------+--------+----+-------------+----------+\n",
      "|729911318064136195|2021-01-04 03:52:20|32033.83| buy|   634.269834|    0.0198|\n",
      "|729934147660808194|2021-01-04 05:23:03| 28700.0|sell|      1854.02|    0.0646|\n",
      "|732579646188552210|2021-01-11 12:35:19|33098.71| buy|   433.593101|    0.0131|\n",
      "|732902582430728192|2021-01-12 09:58:33|33408.74|sell|    651.47043|    0.0195|\n",
      "|733214286326792193|2021-01-13 06:37:09|35196.17| buy|   239.333956|    0.0068|\n",
      "|734663925869576193|2021-01-17 06:37:30|35120.83| buy|    17.560415|    5.0E-4|\n",
      "|735050342902792192|2021-01-18 08:12:59|36877.94| buy|   999.392174|    0.0271|\n",
      "|735076490193928193|2021-01-18 09:56:53|36400.01| buy|    47.320013|    0.0013|\n",
      "|736144355798024193|2021-01-21 08:40:12| 32561.0| buy|     182.3416|    0.0056|\n",
      "|736406051007496192|2021-01-22 02:00:05|31556.51|sell|   211.428617|    0.0067|\n",
      "+------------------+-------------------+--------+----+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Filtering Out Invalid Rows\n",
    "# Assuming price cannot be negative and volume must be greater than zero\n",
    "invalid_rows_count = df_converted.filter((col(\"price\") < 0) | (col(\"volume(quote)\") <= 0)).count()\n",
    "print(f\"Number of invalid rows found: {invalid_rows_count}\")\n",
    "\n",
    "# Filter out invalid rows\n",
    "df_valid = df_converted.filter((col(\"price\") >= 0) & (col(\"volume(quote)\") > 0))\n",
    "\n",
    "# Show row count after filtering\n",
    "valid_rows_count = df_valid.count()\n",
    "print(f\"Row count after filtering invalid rows: {valid_rows_count}\")\n",
    "df_converted.show(10)  # Display the DataFrame as a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875619f2-0403-4a91-98d3-68b839a9a262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows after normalization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+-------------+----------+----+-------------------+---------------------+---------------------+\n",
      "|trade_id          |timestamp          |price   |volume(quote)|size(base)|side|scaledPrice        |scaledVolume         |scaledSize           |\n",
      "+------------------+-------------------+--------+-------------+----------+----+-------------------+---------------------+---------------------+\n",
      "|729911318064136195|2021-01-04 03:52:20|32033.83|634.269834   |0.0198    |buy |0.3093404058307332 |1.0255716472090987E-4|7.7799377014779E-5   |\n",
      "|729934147660808194|2021-01-04 05:23:03|28700.0 |1854.02      |0.0646    |sell|0.24698980289359698|2.9979829285690056E-4|2.5383918156359967E-4|\n",
      "|732579646188552210|2021-01-11 12:35:19|33098.71|433.593101   |0.0131    |buy |0.32925621139241373|7.010649874775602E-5 |5.1471995530915214E-5|\n",
      "|732902582430728192|2021-01-12 09:58:33|33408.74|651.47043    |0.0195    |sell|0.3350545146738049 |1.0533860722792521E-4|7.662053903788958E-5 |\n",
      "|733214286326792193|2021-01-13 06:37:09|35196.17|239.333956   |0.0068    |buy |0.36848373518232674|3.8693596353010224E-5|2.6716398016237305E-5|\n",
      "|734663925869576193|2021-01-17 06:37:30|35120.83|17.560415    |5.0E-4    |buy |0.36707469680146043|2.831446351422617E-6 |1.960800501559408E-6 |\n",
      "|735050342902792192|2021-01-18 08:12:59|36877.94|999.392174   |0.0271    |buy |0.3999368607170984 |1.6159970003404116E-4|1.0648443445242165E-4|\n",
      "|735076490193928193|2021-01-18 09:56:53|36400.01|47.320013    |0.0013    |buy |0.39099842544543023|7.643756975361955E-6 |5.104368439931205E-6 |\n",
      "|736144355798024193|2021-01-21 08:40:12|32561.0 |182.3416     |0.0056    |buy |0.3191997469940109 |2.9477580724636045E-5|2.2001046108679613E-5|\n",
      "|736406051007496192|2021-01-22 02:00:05|31556.51|211.428617   |0.0067    |sell|0.30041337901954185|3.4181130850810605E-5|2.6323452023940836E-5|\n",
      "+------------------+-------------------+--------+-------------+----------+----+-------------------+---------------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Step 1: Assemble numeric columns into a feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"price\", \"volume(quote)\", \"size(base)\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_features = assembler.transform(df_converted)\n",
    "\n",
    "# Step 2: Apply MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "scaled_data = scaler_model.transform(df_features)\n",
    "\n",
    "# Step 3: Convert vector to array and extract individual values\n",
    "scaled_df = scaled_data.withColumn(\n",
    "    \"scaled_array\", vector_to_array(\"scaledFeatures\")\n",
    ").select(\n",
    "    \"trade_id\", \"timestamp\", \"price\", \"volume(quote)\", \"size(base)\", \"side\",\n",
    "    col(\"scaled_array\")[0].alias(\"scaledPrice\"),\n",
    "    col(\"scaled_array\")[1].alias(\"scaledVolume\"),\n",
    "    col(\"scaled_array\")[2].alias(\"scaledSize\")\n",
    ")\n",
    "\n",
    "# Step 4: Display the normalized table with the first 10 rows\n",
    "print(\"First 10 rows after normalization:\")\n",
    "scaled_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84010211-6443-406b-af40-d60d6c6093c6",
   "metadata": {},
   "source": [
    "## 3. Data Analysis Using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f2e087b-066e-46d6-9b88-c965dd398818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "scaled_df.createOrReplaceTempView(\"trades\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af488938-395f-4ec3-afa0-1399ccf32b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:===================================>                   (22 + 11) / 34]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "|        avg_price|  avg_volume_quote|      avg_size_base|      stddev_price|stddev_volume_quote|  stddev_size_base|\n",
      "+-----------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "|33051.15537243369|4012.0078153470267|0.14212028672807997|11497.223661123615|  7913.010948233298|0.3057933820355615|\n",
      "+-----------------+------------------+-------------------+------------------+-------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics for numerical columns\n",
    "summary_stats = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    AVG(price) AS avg_price,\n",
    "    AVG(`volume(quote)`) AS avg_volume_quote,  -- Use backticks to reference columns with special characters\n",
    "    AVG(`size(base)`) AS avg_size_base,        -- Same here for size(base)\n",
    "    STDDEV(price) AS stddev_price,\n",
    "    STDDEV(`volume(quote)`) AS stddev_volume_quote,\n",
    "    STDDEV(`size(base)`) AS stddev_size_base\n",
    "FROM trades\n",
    "\"\"\")\n",
    "\n",
    "# Show the summary statistics\n",
    "summary_stats.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5c07302-2e4a-4b3f-97d0-d195324929fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:===================================>                   (22 + 11) / 34]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|side|  total_volume_quote|         avg_price|\n",
      "+----+--------------------+------------------+\n",
      "| buy|1.048673583319543...|33117.025192550886|\n",
      "|sell|1.051432128143213...| 32985.63472709124|\n",
      "+----+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Grouping and filtering: Group data by specific categories and calculate aggregations foreach group.\n",
    "# Group by 'side' and calculate the total volume for each side\n",
    "side_stats = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    side, \n",
    "    SUM(`volume(quote)`) AS total_volume_quote,\n",
    "    AVG(price) AS avg_price\n",
    "FROM trades\n",
    "GROUP BY side\n",
    "HAVING total_volume_quote > 1000  -- Example filter condition\n",
    "\"\"\")\n",
    "\n",
    "# Show the grouped stats\n",
    "side_stats.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8758d6-7d67-4326-9931-34245bda7827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:======================================>                (24 + 10) / 34]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|trade_date|   avg_daily_price|\n",
      "+----------+------------------+\n",
      "|2020-12-31|29192.522222222222|\n",
      "|2021-01-01|29235.046451612903|\n",
      "|2021-01-02| 31085.51416666667|\n",
      "|2021-01-03| 33514.34820512821|\n",
      "|2021-01-04|31233.909856115108|\n",
      "|2021-01-05| 32658.13606557377|\n",
      "|2021-01-06| 35513.20644230769|\n",
      "|2021-01-07| 38173.10241935485|\n",
      "|2021-01-08| 40384.07938271606|\n",
      "|2021-01-09|        40236.8292|\n",
      "|2021-01-10|          37931.26|\n",
      "|2021-01-11|33892.100806451606|\n",
      "|2021-01-12|34299.704556962024|\n",
      "|2021-01-13| 35270.78153846154|\n",
      "|2021-01-14| 38840.59712121212|\n",
      "|2021-01-15| 36712.81170454545|\n",
      "|2021-01-16|37088.703333333346|\n",
      "|2021-01-17|         35435.637|\n",
      "|2021-01-18| 36612.22265822785|\n",
      "|2021-01-19|36754.923571428575|\n",
      "+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#time based analysis\n",
    "# Analyze daily trends of average price over time\n",
    "daily_trends = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) AS trade_date, \n",
    "    AVG(price) AS avg_daily_price\n",
    "FROM trades\n",
    "GROUP BY trade_date\n",
    "ORDER BY trade_date\n",
    "\"\"\")\n",
    "\n",
    "# Show the daily trends\n",
    "daily_trends.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa302f-16f4-4d61-8afd-6dbf52f0b8d8",
   "metadata": {},
   "source": [
    "#### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d75e201-e839-4382-b2e8-f8889d296498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+------------+---------+----+-------------------+--------------------+--------------------+\n",
      "|          trade_id|          timestamp|   price|volume_quote|size_base|side|        scaledPrice|        scaledVolume|          scaledSize|\n",
      "+------------------+-------------------+--------+------------+---------+----+-------------------+--------------------+--------------------+\n",
      "|729911318064136195|2021-01-04 03:52:20|32033.83|  634.269834|   0.0198| buy| 0.3093404058307332|1.025571647209098...|  7.7799377014779E-5|\n",
      "|729934147660808194|2021-01-04 05:23:03| 28700.0|     1854.02|   0.0646|sell|0.24698980289359698|2.997982928569005...|2.538391815635996...|\n",
      "|732579646188552210|2021-01-11 12:35:19|33098.71|  433.593101|   0.0131| buy|0.32925621139241373|7.010649874775602E-5|5.147199553091521...|\n",
      "|732902582430728192|2021-01-12 09:58:33|33408.74|   651.47043|   0.0195|sell| 0.3350545146738049|1.053386072279252...|7.662053903788958E-5|\n",
      "|733214286326792193|2021-01-13 06:37:09|35196.17|  239.333956|   0.0068| buy|0.36848373518232674|3.869359635301022...|2.671639801623730...|\n",
      "|734663925869576193|2021-01-17 06:37:30|35120.83|   17.560415|   5.0E-4| buy|0.36707469680146043|2.831446351422617E-6|1.960800501559408E-6|\n",
      "|735050342902792192|2021-01-18 08:12:59|36877.94|  999.392174|   0.0271| buy| 0.3999368607170984|1.615997000340411...|1.064844344524216...|\n",
      "|735076490193928193|2021-01-18 09:56:53|36400.01|   47.320013|   0.0013| buy|0.39099842544543023|7.643756975361955E-6|5.104368439931205E-6|\n",
      "|736144355798024193|2021-01-21 08:40:12| 32561.0|    182.3416|   0.0056| buy| 0.3191997469940109|2.947758072463604...|2.200104610867961...|\n",
      "|736406051007496192|2021-01-22 02:00:05|31556.51|  211.428617|   0.0067|sell|0.30041337901954185|3.418113085081060...|2.632345202394083...|\n",
      "+------------------+-------------------+--------+------------+---------+----+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Trade Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"trade_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"volume_quote\", DoubleType(), True),\n",
    "    StructField(\"size_base\", DoubleType(), True),\n",
    "    StructField(\"side\", StringType(), True),\n",
    "    StructField(\"scaledPrice\", DoubleType(), True),\n",
    "    StructField(\"scaledVolume\", DoubleType(), True),\n",
    "    StructField(\"scaledSize\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"729911318064136195\", \"2021-01-04 03:52:20\", 32033.83, 634.269834, 0.0198, \"buy\", 0.3093404058307332, 1.0255716472090987E-4, 7.7799377014779E-5),\n",
    "    (\"729934147660808194\", \"2021-01-04 05:23:03\", 28700.0, 1854.02, 0.0646, \"sell\", 0.24698980289359698, 2.9979829285690056E-4, 2.5383918156359967E-4),\n",
    "    (\"732579646188552210\", \"2021-01-11 12:35:19\", 33098.71, 433.593101, 0.0131, \"buy\", 0.32925621139241373, 7.010649874775602E-5, 5.1471995530915214E-5),\n",
    "    (\"732902582430728192\", \"2021-01-12 09:58:33\", 33408.74, 651.47043, 0.0195, \"sell\", 0.3350545146738049, 1.0533860722792521E-4, 7.662053903788958E-5),\n",
    "    (\"733214286326792193\", \"2021-01-13 06:37:09\", 35196.17, 239.333956, 0.0068, \"buy\", 0.36848373518232674, 3.8693596353010224E-5, 2.6716398016237305E-5),\n",
    "    (\"734663925869576193\", \"2021-01-17 06:37:30\", 35120.83, 17.560415, 5.0E-4, \"buy\", 0.36707469680146043, 2.831446351422617E-6, 1.960800501559408E-6),\n",
    "    (\"735050342902792192\", \"2021-01-18 08:12:59\", 36877.94, 999.392174, 0.0271, \"buy\", 0.3999368607170984, 1.6159970003404116E-4, 1.0648443445242165E-4),\n",
    "    (\"735076490193928193\", \"2021-01-18 09:56:53\", 36400.01, 47.320013, 0.0013, \"buy\", 0.39099842544543023, 7.643756975361955E-6, 5.104368439931205E-6),\n",
    "    (\"736144355798024193\", \"2021-01-21 08:40:12\", 32561.0, 182.3416, 0.0056, \"buy\", 0.3191997469940109, 2.9477580724636045E-5, 2.2001046108679613E-5),\n",
    "    (\"736406051007496192\", \"2021-01-22 02:00:05\", 31556.51, 211.428617, 0.0067, \"sell\", 0.30041337901954185, 3.4181130850810605E-5, 2.6323452023940836E-5),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "trades_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "trades_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13af00d-8e96-47be-ab2d-7777f0f52aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|          trade_id|user_id|\n",
      "+------------------+-------+\n",
      "|729911318064136195|  user1|\n",
      "|729934147660808194|  user2|\n",
      "|732579646188552210|  user1|\n",
      "|732902582430728192|  user3|\n",
      "|733214286326792193|  user2|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample user data\n",
    "user_data = [\n",
    "    (\"729911318064136195\", \"user1\"),\n",
    "    (\"729934147660808194\", \"user2\"),\n",
    "    (\"732579646188552210\", \"user1\"),\n",
    "    (\"732902582430728192\", \"user3\"),\n",
    "    (\"733214286326792193\", \"user2\"),\n",
    "]\n",
    "\n",
    "# Define schema for user data\n",
    "user_schema = StructType([\n",
    "    StructField(\"trade_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Create DataFrame for user data\n",
    "user_df = spark.createDataFrame(user_data, user_schema)\n",
    "\n",
    "# Show the user DataFrame\n",
    "user_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc04120-0ff6-435c-8bb1-7190a385c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------+------------+---------+----+-------------------+--------------------+--------------------+-------+\n",
      "|          trade_id|          timestamp|   price|volume_quote|size_base|side|        scaledPrice|        scaledVolume|          scaledSize|user_id|\n",
      "+------------------+-------------------+--------+------------+---------+----+-------------------+--------------------+--------------------+-------+\n",
      "|729911318064136195|2021-01-04 03:52:20|32033.83|  634.269834|   0.0198| buy| 0.3093404058307332|1.025571647209098...|  7.7799377014779E-5|  user1|\n",
      "|729934147660808194|2021-01-04 05:23:03| 28700.0|     1854.02|   0.0646|sell|0.24698980289359698|2.997982928569005...|2.538391815635996...|  user2|\n",
      "|732579646188552210|2021-01-11 12:35:19|33098.71|  433.593101|   0.0131| buy|0.32925621139241373|7.010649874775602E-5|5.147199553091521...|  user1|\n",
      "|732902582430728192|2021-01-12 09:58:33|33408.74|   651.47043|   0.0195|sell| 0.3350545146738049|1.053386072279252...|7.662053903788958E-5|  user3|\n",
      "|733214286326792193|2021-01-13 06:37:09|35196.17|  239.333956|   0.0068| buy|0.36848373518232674|3.869359635301022...|2.671639801623730...|  user2|\n",
      "+------------------+-------------------+--------+------------+---------+----+-------------------+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform the join\n",
    "joined_df = trades_df.join(user_df, on=\"trade_id\", how=\"inner\")\n",
    "\n",
    "# Show the joined DataFrame\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bb79c-2442-4fe1-aaf2-6f34e44aeff6",
   "metadata": {},
   "source": [
    "#### 4. Machine Learning Model (Regression/Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50eaf861-6c2b-4e54-9086-2dc32aa2b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc157748-0a33-44d4-ac58-e7d37c0c89e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 08:47:42 WARN Utils: Your hostname, Anmols-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.4.2.150 instead (on interface en0)\n",
      "24/11/05 08:47:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 08:47:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 1:==================================================>      (23 + 3) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trade_id: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- side: string (nullable = true)\n",
      " |-- volume(quote): double (nullable = true)\n",
      " |-- size(base): double (nullable = true)\n",
      "\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "|          trade_id|    timestamp|   price|side|volume(quote)|size(base)|\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "|728569744763912192|1609430484000|28814.99| buy|   2103.49427|     0.073|\n",
      "|728574157171720192|1609431536000| 28627.2| buy|     74.43072|    0.0026|\n",
      "|728708911770632192|1609463664000| 29335.0|sell|    1041.3925|    0.0355|\n",
      "|728711638068232192|1609464314000|29301.46| buy|    102.55511|    0.0035|\n",
      "|728713911381000192|1609464856000|29345.12| buy|     88.03536|     0.003|\n",
      "|728720152505352192|1609466344000|29390.18| buy|   111.682684|    0.0038|\n",
      "|728725932256264192|1609467722000|29310.74| buy|  2142.615094|    0.0731|\n",
      "|728726825643016192|1609467935000|29348.01| buy|     2.934801|    1.0E-4|\n",
      "|728741895777288192|1609471528000| 29260.0|sell|      318.934|    0.0109|\n",
      "|728766495370248192|1609477393000|29171.31| buy|   107.933847|    0.0037|\n",
      "+------------------+-------------+--------+----+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TradeSideClassification\").config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.memory\", \"4g\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"bitcoin_transactions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Check schema to ensure correct data types\n",
    "df.printSchema()\n",
    "\n",
    "# Show a sample of the data\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30247ab-c219-4d84-a9ba-f9ef795650a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the feature columns that exist in the dataset\n",
    "feature_columns = [\"price\", \"volume(quote)\", \"size(base)\"]\n",
    "label_column = \"side\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1661f1de-483b-4364-8e80-ca1e88cb8ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 08:47:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert the `side` column from string to numeric index\n",
    "indexer = StringIndexer(inputCol=label_column, outputCol=\"label\")\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Re-split the indexed data into training and testing sets\n",
    "train_data, test_data = indexed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 2: Combine features into a single vector\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19458a25-38f4-457f-98e7-5d5f99bf7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the Logistic Regression model\n",
    "#logistic_regression = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "logistic_regression = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=5)\n",
    "\n",
    "\n",
    "# Step 4: Create a pipeline to streamline the process\n",
    "pipeline = Pipeline(stages=[vector_assembler, logistic_regression])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2fdf1d-b9cd-4770-b710-f6a30fbc0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_sample = train_data.sample(withReplacement=False, fraction=0.1, seed=42)  # 10% sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "856de2a7-2ff2-42d0-a68f-11bf02e03f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 08:48:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/11/05 08:48:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data_sample = train_data.sample(fraction=0.1, seed=42)  # Adjust fraction as needed\n",
    "test_data_sample = test_data.sample(fraction=0.1, seed=42)\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data_sample)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.transform(test_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86939690-4022-492f-87ff-cb807c4cfe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=================================================>      (23 + 3) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5011\n",
      "F1 Score: 0.4979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize evaluators for accuracy and F1-score\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Calculate accuracy and F1-score\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5a7e6-5485-46ae-840d-f177f2b904ab",
   "metadata": {},
   "source": [
    "## 5. Model Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9d22878-f1ad-4aae-96a1-c4b9c7ce4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Step 1: Define a parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(logistic_regression.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(logistic_regression.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(logistic_regression.maxIter, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0955a02-c0f5-4865-99fc-ab64dde013af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set up the evaluator (we'll use F1-score as the primary metric)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Step 3: Initialize cross-validator\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5  # Set the number of folds for cross-validation\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbcf088b-f314-43c4-a666-36e5a55cd690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Fit the cross-validator to the training data\n",
    "cv_model = cross_validator.fit(train_data_sample)\n",
    "\n",
    "# Step 5: Make predictions on the test set with the best model from cross-validation\n",
    "best_model_predictions = cv_model.transform(test_data_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cb9a60e-0faf-4982-8d6d-e553d4a336df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1572:===============================================>      (23 + 3) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Evaluation Metrics:\n",
      "Accuracy: 0.5011\n",
      "F1 Score: 0.4989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate final model on the test data\n",
    "accuracy = accuracy_evaluator.evaluate(best_model_predictions)\n",
    "f1_score = f1_evaluator.evaluate(best_model_predictions)\n",
    "\n",
    "# Print final evaluation metrics\n",
    "print(\"Final Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
